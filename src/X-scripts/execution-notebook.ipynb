{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e092082",
   "metadata": {},
   "source": [
    "## X) Test load .WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d3709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:43: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  backend = torchaudio.get_audio_backend()\n",
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:45: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import (\n",
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:53: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(backend)\n",
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\tasks\\segmentation\\mixins.py:37: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n",
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00:00:00.636 -->  00:00:05.237] A SPEAKER_00\n",
      "[ 00:00:06.052 -->  00:00:08.616] B SPEAKER_00\n",
      "[ 00:00:09.091 -->  00:00:13.047] C SPEAKER_00\n",
      "[ 00:00:13.455 -->  00:00:18.259] D SPEAKER_00\n",
      "[ 00:00:18.752 -->  00:00:21.689] E SPEAKER_00\n",
      "[ 00:00:21.723 -->  00:00:22.555] F SPEAKER_00\n",
      "00  0.64‚Äì5.24s  track=A  -> SPEAKER_00\n",
      "01  6.05‚Äì8.62s  track=B  -> SPEAKER_00\n",
      "02  9.09‚Äì13.05s  track=C  -> SPEAKER_00\n",
      "03  13.46‚Äì18.26s  track=D  -> SPEAKER_00\n",
      "04  18.75‚Äì21.69s  track=E  -> SPEAKER_00\n",
      "05  21.72‚Äì22.56s  track=F  -> SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "# quick_test.py\n",
    "from dotenv import load_dotenv\n",
    "import os, torch\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "assert token and token.startswith(\"hf_\")\n",
    "\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=token,\n",
    ")\n",
    "# Run on your short file\n",
    "diar = pipeline(\"A-data/1-raw/1-2-speaker-enroll/001-Justin-Anderson/Justin-mini.wav\")\n",
    "print(diar)\n",
    "#show segments\n",
    "for i, (seg, track, spk) in enumerate(diar.itertracks(yield_label=True)):\n",
    "    print(f\"{i:02d}  {seg.start:.2f}‚Äì{seg.end:.2f}s  track={track}  -> {spk}\")\n",
    "    if i >= 5: break\n",
    "\n",
    "# show first few segments\n",
    "#for i, ((seg, _), label) in enumerate(diar.itertracks(yield_label=True)):\n",
    " #   print(f\"{i:02d}  {seg.start:.2f}‚Äì{seg.end:.2f}s  -> {label}\")\n",
    "  #  if i >= 5: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9a82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00  0.64‚Äì5.24s  track=A  -> SPEAKER_00\n",
      "01  6.05‚Äì8.62s  track=B  -> SPEAKER_00\n",
      "02  9.09‚Äì13.05s  track=C  -> SPEAKER_00\n",
      "03  13.46‚Äì18.26s  track=D  -> SPEAKER_00\n",
      "04  18.75‚Äì21.69s  track=E  -> SPEAKER_00\n",
      "05  21.72‚Äì22.56s  track=F  -> SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "for i, (seg, track, spk) in enumerate(diar.itertracks(yield_label=True)):\n",
    "    print(f\"{i:02d}  {seg.start:.2f}‚Äì{seg.end:.2f}s  track={track}  -> {spk}\")\n",
    "    #if i >= 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad17c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00:00:00.636 -->  00:00:05.237] A SPEAKER_00\n",
      "[ 00:00:06.052 -->  00:00:08.616] B SPEAKER_00\n",
      "[ 00:00:09.091 -->  00:00:13.047] C SPEAKER_00\n",
      "[ 00:00:13.455 -->  00:00:18.259] D SPEAKER_00\n",
      "[ 00:00:18.752 -->  00:00:21.689] E SPEAKER_00\n",
      "[ 00:00:21.723 -->  00:00:22.555] F SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "print(diar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d1df21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Annotation.itertracks of <pyannote.core.annotation.Annotation object at 0x000001E0734ADB50>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diar.itertracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee6d7c",
   "metadata": {},
   "source": [
    "## 3) Build a manifest CSV (path,label,duration_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29062890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Wrote A-data/2-processed/2-2-manifests/all.csv with 2511 rows\n"
     ]
    }
   ],
   "source": [
    "#3) Build a manifest CSV (path,label,duration_s)\n",
    "\n",
    "import os, csv, librosa\n",
    "\n",
    "WAVROOT = \"A-data/1-raw/1-1-datasets\" ## Just take from raw cause' 50-person dataset already in WAV 16khz files (ignore filename issues)\n",
    "OUTCSV  = \"A-data/2-processed/2-2-manifests/all.csv\"\n",
    "\n",
    "rows = []\n",
    "for root, _, files in os.walk(WAVROOT):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".wav\"):\n",
    "            path = os.path.join(root, f)\n",
    "            # label = immediate parent folder\n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            try:\n",
    "                dur = librosa.get_duration(path=path)\n",
    "            except:\n",
    "                dur = 0.0\n",
    "            rows.append((path, label, f\"{dur:.2f}\"))\n",
    "\n",
    "rows.sort()\n",
    "os.makedirs(os.path.dirname(OUTCSV), exist_ok=True)\n",
    "with open(OUTCSV, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    w.writerow([\"path\",\"label\",\"duration_s\"])\n",
    "    w.writerows(rows)\n",
    "print(f\"‚úì Wrote {OUTCSV} with {len(rows)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3c70a",
   "metadata": {},
   "source": [
    "## 3b) Create splits (train/val/test) first\n",
    "\n",
    "Definitions (with analogy): \n",
    "Train ‚Üí what the model actually learns from (study material).\n",
    "Validation (val) ‚Üí what you use to tune and choose the model (practice quiz you can take multiple times to adjust your study strategy).\n",
    "Test ‚Üí what you keep hidden until the very end, to report final performance (the real exam you only take once).\n",
    "\n",
    "#### Why do we need validation (val)?\n",
    "\n",
    "When you train, you often have knobs to adjust:\n",
    "* Which algorithm (LogReg vs SVM)\n",
    "* Hyperparameters (e.g., C=2.0 vs C=0.1 in LogisticRegression)\n",
    "* Data prep choices (balance classes? scale differently?)\n",
    "* Early stopping (stop training when performance stops improving)\n",
    "\n",
    "If you use the test set to make those choices, you ‚Äúpeek‚Äù at your exam answers. The model will look artificially good, but you‚Äôve actually tuned it to that test set.\n",
    "\n",
    "So instead:\n",
    "\n",
    "1. Train on train.\n",
    "2. Evaluate on val while you adjust hyperparameters.\n",
    "3. Once you‚Äôve decided the ‚Äúbest‚Äù setup ‚Üí lock it down.\n",
    "\n",
    "Run one final report on test.\n",
    "\n",
    "This way:\n",
    "\n",
    "* val guides development.\n",
    "\n",
    "* test measures generalization fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "320fac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Wrote A-data/2-processed/2-2-manifests/split.csv with 2511 rows\n"
     ]
    }
   ],
   "source": [
    "## 3b) Create splits (train/val/test) first\n",
    "\n",
    "import csv, os, random, collections\n",
    "\n",
    "ALL_CSV  = \"A-data/2-processed/2-2-manifests/all.csv\"\n",
    "SPLIT_CSV= \"A-data/2-processed/2-2-manifests/split.csv\"\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# group by label\n",
    "by_label = collections.defaultdict(list)\n",
    "with open(ALL_CSV, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    rdr = csv.DictReader(fh)\n",
    "    for r in rdr:\n",
    "        by_label[r[\"label\"]].append(r[\"path\"])\n",
    "\n",
    "rows = []\n",
    "for label, paths in by_label.items():\n",
    "    random.shuffle(paths)\n",
    "    n = len(paths)\n",
    "    # 80/10/10 split, but be robust for tiny classes\n",
    "    n_val  = max(1, int(0.10 * n)) if n >= 10 else max(0, int(0.10 * n))\n",
    "    n_test = max(1, int(0.10 * n)) if n >= 10 else max(0, int(0.10 * n))\n",
    "    # keep at least 1 in train if possible\n",
    "    if n - (n_val + n_test) <= 0:\n",
    "        n_val = 0\n",
    "        n_test = min(1, n-1) if n > 1 else 0\n",
    "    val  = set(paths[:n_val])\n",
    "    test = set(paths[n_val:n_val+n_test])\n",
    "    for p in paths:\n",
    "        split = \"train\"\n",
    "        if p in val:  split = \"val\"\n",
    "        elif p in test: split = \"test\"\n",
    "        rows.append({\"path\": p, \"split\": split})\n",
    "\n",
    "os.makedirs(os.path.dirname(SPLIT_CSV), exist_ok=True)\n",
    "with open(SPLIT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.DictWriter(fh, fieldnames=[\"path\",\"split\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "print(f\"‚úì Wrote {SPLIT_CSV} with {len(rows)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f8dd9",
   "metadata": {},
   "source": [
    "### Check the \"split\" (train/val/test) amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aad61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    2051\n",
      "val       230\n",
      "test      230\n",
      "Name: count, dtype: int64\n",
      "split\n",
      "train    81.680605\n",
      "val       9.159697\n",
      "test      9.159697\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"A-data/2-processed/2-2-manifests/split.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Count raw counts\n",
    "print(df[\"split\"].value_counts())\n",
    "\n",
    "# Count percentages\n",
    "print(df[\"split\"].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec139a",
   "metadata": {},
   "source": [
    "## 4) Extract ECAPA embeddings - had to \"Turn on Windows ‚ÄúDeveloper Mode‚Äù (fastest, 1-time OS setting)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117c5b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n",
      "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2511/2511 [03:13<00:00, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ECAPA embeddings saved under: B-work/1-dataset-ecapa-embeds\n",
      "‚úì Index written: B-work/1-dataset-ecapa-embeds\\index.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Extract ECAPA embeddings\n",
    "\n",
    "import os, csv, numpy as np, torch, torchaudio, pathlib\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "MANIFEST   = \"A-data/2-processed/2-2-manifests/all.csv\"\n",
    "SPLIT_FILE = \"A-data/2-processed/2-2-manifests/split.csv\"\n",
    "OUTDIR     = \"B-work/1-dataset-ecapa-embeds\"\n",
    "MODEL_DIR  = \"B-work/0-ecapa-model-cache\"  # local snapshot folder\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# load split mapping\n",
    "path2split = {}\n",
    "with open(SPLIT_FILE, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    for r in csv.DictReader(fh):\n",
    "        path2split[r[\"path\"]] = r[\"split\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "enc = EncoderClassifier.from_hparams(\n",
    "    source=MODEL_DIR,         # load from local folder\n",
    "    savedir=MODEL_DIR,        # keep caches here\n",
    "    run_opts={\"device\": device}\n",
    ")\n",
    "\n",
    "def load_mono16k(path):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    return wav\n",
    "\n",
    "paths, labels = [], []\n",
    "with open(MANIFEST, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    rdr = csv.DictReader(fh)\n",
    "    for r in rdr:\n",
    "        paths.append(r[\"path\"])\n",
    "        labels.append(r[\"label\"])\n",
    "\n",
    "out_index = []\n",
    "for p, y in tqdm(list(zip(paths, labels)), desc=\"Embedding\"):\n",
    "    wav = load_mono16k(p).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = enc.encode_batch(wav).squeeze().cpu().numpy()\n",
    "    spkdir = os.path.join(OUTDIR, y)\n",
    "    pathlib.Path(spkdir).mkdir(parents=True, exist_ok=True)\n",
    "    outnpy = os.path.join(spkdir, pathlib.Path(p).stem + \".npy\")\n",
    "    np.save(outnpy, emb)\n",
    "    split = path2split.get(p, \"train\")  # default safe fallback\n",
    "    out_index.append((outnpy, y, p, split))\n",
    "\n",
    "# write index with split + original path\n",
    "index_csv = os.path.join(OUTDIR, \"index.csv\")\n",
    "with open(index_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.writer(fh)\n",
    "    w.writerow([\"npy\",\"label\",\"src_path\",\"split\"])\n",
    "    w.writerows(out_index)\n",
    "\n",
    "print(\"‚úì ECAPA embeddings saved under:\", OUTDIR)\n",
    "print(\"‚úì Index written:\", index_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2eb869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ECAPA ‚úî\n"
     ]
    }
   ],
   "source": [
    "# sanity test to make sure it can connect\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"  # force copy instead of symlink on Windows\n",
    "\n",
    "\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "enc = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"B-work/0-ecapa-model-cache\",\n",
    "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    ")\n",
    "print(\"Loaded ECAPA ‚úî\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac92ae",
   "metadata": {},
   "source": [
    "## 5) CANCELLED (break by test): Train a simple classifier (LogReg / LinearSVC) + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413dccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Classification Report ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Speaker0026       1.00      0.89      0.94         9\n",
      " Speaker0027       1.00      1.00      1.00         9\n",
      " Speaker0028       1.00      1.00      1.00        12\n",
      " Speaker0029       1.00      1.00      1.00         6\n",
      " Speaker0030       1.00      1.00      1.00         7\n",
      " Speaker0031       1.00      1.00      1.00         9\n",
      " Speaker0032       1.00      1.00      1.00         7\n",
      " Speaker0033       1.00      1.00      1.00         7\n",
      " Speaker0034       1.00      1.00      1.00         7\n",
      " Speaker0035       1.00      1.00      1.00         6\n",
      " Speaker0036       1.00      1.00      1.00         7\n",
      " Speaker0037       1.00      1.00      1.00        11\n",
      " Speaker0038       0.70      1.00      0.82         7\n",
      " Speaker0039       1.00      1.00      1.00        10\n",
      " Speaker0040       1.00      1.00      1.00         6\n",
      " Speaker0041       1.00      1.00      1.00         6\n",
      " Speaker0042       1.00      1.00      1.00         8\n",
      " Speaker0043       1.00      0.86      0.92         7\n",
      " Speaker0044       1.00      1.00      1.00         7\n",
      " Speaker0045       1.00      1.00      1.00         8\n",
      " Speaker0046       1.00      1.00      1.00         8\n",
      " Speaker0047       1.00      0.86      0.92         7\n",
      " Speaker0048       1.00      0.83      0.91         6\n",
      " Speaker0049       1.00      1.00      1.00        10\n",
      " Speaker0050       1.00      1.00      1.00         7\n",
      "Speaker_0000       1.00      1.00      1.00        19\n",
      "Speaker_0001       1.00      1.00      1.00        24\n",
      "Speaker_0002       0.92      0.96      0.94        24\n",
      "Speaker_0003       1.00      0.94      0.97        17\n",
      "Speaker_0004       1.00      0.94      0.97        18\n",
      "Speaker_0005       1.00      1.00      1.00        14\n",
      "Speaker_0006       0.93      0.87      0.90        15\n",
      "Speaker_0007       1.00      1.00      1.00        12\n",
      "Speaker_0008       1.00      1.00      1.00        20\n",
      "Speaker_0009       0.87      1.00      0.93        13\n",
      "Speaker_0010       1.00      1.00      1.00        11\n",
      "Speaker_0011       1.00      0.92      0.96        13\n",
      "Speaker_0012       1.00      1.00      1.00         7\n",
      "Speaker_0013       0.78      1.00      0.88         7\n",
      "Speaker_0014       1.00      1.00      1.00        10\n",
      "Speaker_0015       1.00      1.00      1.00         8\n",
      "Speaker_0016       1.00      1.00      1.00        10\n",
      "Speaker_0017       1.00      1.00      1.00         9\n",
      "Speaker_0018       0.89      1.00      0.94         8\n",
      "Speaker_0019       1.00      1.00      1.00         7\n",
      "Speaker_0020       1.00      1.00      1.00         2\n",
      "Speaker_0021       1.00      0.92      0.96        12\n",
      "Speaker_0023       1.00      1.00      1.00         8\n",
      "Speaker_0024       1.00      1.00      1.00        11\n",
      "Speaker_0025       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       503\n",
      "   macro avg       0.98      0.98      0.98       503\n",
      "weighted avg       0.98      0.98      0.98       503\n",
      "\n",
      "== Confusion Matrix ==\n",
      "[[ 8  0  0 ...  0  0  0]\n",
      " [ 0  9  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  8  0  0]\n",
      " [ 0  0  0 ...  0 11  0]\n",
      " [ 0  0  0 ...  0  0 10]]\n",
      "‚úì Saved C-models/speaker_id_model.joblib\n"
     ]
    }
   ],
   "source": [
    "## 5) Train a simple classifier (LogReg / LinearSVC) + metrics\n",
    "## 5) Train a simple classifier (LogReg / LinearSVC) + metrics\n",
    "\n",
    "import os, csv, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "INDEX = \"B-work/1-dataset-ecapa-embeds/index.csv\"\n",
    "MODEL_OUT = \"C-models/speaker_id_model.joblib\"\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "# 1) Load embeddings and labels from index.csv\n",
    "with open(INDEX, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    rdr = csv.DictReader(fh)\n",
    "    for r in rdr:\n",
    "        X.append(np.load(r[\"npy\"]))\n",
    "        y.append(r[\"label\"])\n",
    "\n",
    "X = np.vstack(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# 2) Encode string labels ‚Üí integers\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "# 3) Train/test split (stratified)\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# 4) Standardize features\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "Xtr_s = scaler.fit_transform(Xtr)\n",
    "Xte_s = scaler.transform(Xte)\n",
    "\n",
    "# 5) Train classifier\n",
    "clf = LogisticRegression(max_iter=2000, n_jobs=None, class_weight=\"balanced\")\n",
    "clf.fit(Xtr_s, ytr)\n",
    "\n",
    "# 6) Evaluate\n",
    "yhat = clf.predict(Xte_s)\n",
    "\n",
    "print(\"== Classification Report ==\")\n",
    "print(classification_report(yte, yhat, target_names=le.classes_))\n",
    "print(\"== Confusion Matrix ==\")\n",
    "print(confusion_matrix(yte, yhat))\n",
    "\n",
    "# 7) Save bundle (model + scaler + encoder)\n",
    "os.makedirs(os.path.dirname(MODEL_OUT), exist_ok=True)\n",
    "joblib.dump(\n",
    "    {\"clf\": clf, \"scaler\": scaler, \"label_encoder\": le},\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(f\"‚úì Saved {MODEL_OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3834e9d",
   "metadata": {},
   "source": [
    "## 5a) Train Eval Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "859ce902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== TRAIN ==\n",
      "Accuracy: 0.9990 | Macro-F1: 0.9991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Speaker0026       1.00      1.00      1.00        37\n",
      " Speaker0027       1.00      1.00      1.00        39\n",
      " Speaker0028       1.00      1.00      1.00        49\n",
      " Speaker0029       1.00      1.00      1.00        25\n",
      " Speaker0030       1.00      1.00      1.00        27\n",
      " Speaker0031       1.00      1.00      1.00        39\n",
      " Speaker0032       1.00      1.00      1.00        31\n",
      " Speaker0033       1.00      1.00      1.00        29\n",
      " Speaker0034       1.00      1.00      1.00        28\n",
      " Speaker0035       1.00      1.00      1.00        26\n",
      " Speaker0036       1.00      1.00      1.00        27\n",
      " Speaker0037       1.00      1.00      1.00        45\n",
      " Speaker0038       1.00      1.00      1.00        28\n",
      " Speaker0039       1.00      1.00      1.00        41\n",
      " Speaker0040       1.00      1.00      1.00        26\n",
      " Speaker0041       1.00      1.00      1.00        26\n",
      " Speaker0042       1.00      1.00      1.00        33\n",
      " Speaker0043       1.00      1.00      1.00        28\n",
      " Speaker0044       1.00      1.00      1.00        31\n",
      " Speaker0045       1.00      1.00      1.00        34\n",
      " Speaker0046       1.00      1.00      1.00        32\n",
      " Speaker0047       1.00      1.00      1.00        28\n",
      " Speaker0048       1.00      1.00      1.00        25\n",
      " Speaker0049       1.00      1.00      1.00        40\n",
      " Speaker0050       1.00      1.00      1.00        27\n",
      "Speaker_0000       1.00      1.00      1.00        75\n",
      "Speaker_0001       1.00      1.00      1.00        96\n",
      "Speaker_0002       1.00      0.99      0.99        96\n",
      "Speaker_0003       1.00      0.99      0.99        69\n",
      "Speaker_0004       1.00      1.00      1.00        72\n",
      "Speaker_0005       1.00      1.00      1.00        58\n",
      "Speaker_0006       1.00      1.00      1.00        62\n",
      "Speaker_0007       1.00      1.00      1.00        50\n",
      "Speaker_0008       1.00      1.00      1.00        81\n",
      "Speaker_0009       1.00      1.00      1.00        53\n",
      "Speaker_0010       1.00      1.00      1.00        44\n",
      "Speaker_0011       1.00      1.00      1.00        53\n",
      "Speaker_0012       1.00      1.00      1.00        28\n",
      "Speaker_0013       0.94      1.00      0.97        31\n",
      "Speaker_0014       1.00      1.00      1.00        41\n",
      "Speaker_0015       1.00      1.00      1.00        32\n",
      "Speaker_0016       1.00      1.00      1.00        40\n",
      "Speaker_0017       1.00      1.00      1.00        36\n",
      "Speaker_0018       1.00      1.00      1.00        32\n",
      "Speaker_0019       1.00      1.00      1.00        27\n",
      "Speaker_0020       1.00      1.00      1.00         8\n",
      "Speaker_0021       1.00      1.00      1.00        48\n",
      "Speaker_0023       1.00      1.00      1.00        33\n",
      "Speaker_0024       1.00      1.00      1.00        44\n",
      "Speaker_0025       1.00      1.00      1.00        41\n",
      "\n",
      "    accuracy                           1.00      2051\n",
      "   macro avg       1.00      1.00      1.00      2051\n",
      "weighted avg       1.00      1.00      1.00      2051\n",
      "\n",
      "\n",
      "== VAL ==\n",
      "Accuracy: 0.9913 | Macro-F1: 0.9958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Speaker0026       1.00      1.00      1.00         4\n",
      " Speaker0027       1.00      1.00      1.00         4\n",
      " Speaker0028       1.00      1.00      1.00         5\n",
      " Speaker0029       1.00      1.00      1.00         3\n",
      " Speaker0030       1.00      1.00      1.00         3\n",
      " Speaker0031       1.00      1.00      1.00         4\n",
      " Speaker0032       1.00      1.00      1.00         3\n",
      " Speaker0033       1.00      1.00      1.00         3\n",
      " Speaker0034       1.00      1.00      1.00         3\n",
      " Speaker0035       1.00      1.00      1.00         3\n",
      " Speaker0036       1.00      1.00      1.00         3\n",
      " Speaker0037       1.00      1.00      1.00         5\n",
      " Speaker0038       1.00      1.00      1.00         3\n",
      " Speaker0039       1.00      1.00      1.00         5\n",
      " Speaker0040       1.00      1.00      1.00         3\n",
      " Speaker0041       1.00      1.00      1.00         3\n",
      " Speaker0042       1.00      1.00      1.00         4\n",
      " Speaker0043       1.00      1.00      1.00         3\n",
      " Speaker0044       1.00      1.00      1.00         3\n",
      " Speaker0045       1.00      1.00      1.00         4\n",
      " Speaker0046       1.00      1.00      1.00         3\n",
      " Speaker0047       1.00      1.00      1.00         3\n",
      " Speaker0048       1.00      1.00      1.00         3\n",
      " Speaker0049       1.00      1.00      1.00         5\n",
      " Speaker0050       1.00      1.00      1.00         3\n",
      "Speaker_0000       1.00      1.00      1.00         9\n",
      "Speaker_0001       1.00      1.00      1.00        11\n",
      "Speaker_0002       0.92      0.92      0.92        12\n",
      "Speaker_0003       1.00      1.00      1.00         8\n",
      "Speaker_0004       1.00      0.89      0.94         9\n",
      "Speaker_0005       1.00      1.00      1.00         7\n",
      "Speaker_0006       0.88      1.00      0.93         7\n",
      "Speaker_0007       1.00      1.00      1.00         6\n",
      "Speaker_0008       1.00      1.00      1.00         9\n",
      "Speaker_0009       1.00      1.00      1.00         6\n",
      "Speaker_0010       1.00      1.00      1.00         5\n",
      "Speaker_0011       1.00      1.00      1.00         6\n",
      "Speaker_0012       1.00      1.00      1.00         3\n",
      "Speaker_0013       1.00      1.00      1.00         3\n",
      "Speaker_0014       1.00      1.00      1.00         5\n",
      "Speaker_0015       1.00      1.00      1.00         3\n",
      "Speaker_0016       1.00      1.00      1.00         4\n",
      "Speaker_0017       1.00      1.00      1.00         4\n",
      "Speaker_0018       1.00      1.00      1.00         4\n",
      "Speaker_0019       1.00      1.00      1.00         3\n",
      "Speaker_0020       1.00      1.00      1.00         1\n",
      "Speaker_0021       1.00      1.00      1.00         5\n",
      "Speaker_0023       1.00      1.00      1.00         3\n",
      "Speaker_0024       1.00      1.00      1.00         5\n",
      "Speaker_0025       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.99       230\n",
      "   macro avg       1.00      1.00      1.00       230\n",
      "weighted avg       0.99      0.99      0.99       230\n",
      "\n",
      "\n",
      "== TEST ==\n",
      "Accuracy: 0.9826 | Macro-F1: 0.9822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Speaker0026       1.00      1.00      1.00         4\n",
      " Speaker0027       1.00      1.00      1.00         4\n",
      " Speaker0028       1.00      1.00      1.00         5\n",
      " Speaker0029       1.00      1.00      1.00         3\n",
      " Speaker0030       1.00      1.00      1.00         3\n",
      " Speaker0031       1.00      1.00      1.00         4\n",
      " Speaker0032       1.00      1.00      1.00         3\n",
      " Speaker0033       1.00      1.00      1.00         3\n",
      " Speaker0034       1.00      1.00      1.00         3\n",
      " Speaker0035       1.00      1.00      1.00         3\n",
      " Speaker0036       1.00      1.00      1.00         3\n",
      " Speaker0037       1.00      1.00      1.00         5\n",
      " Speaker0038       1.00      0.67      0.80         3\n",
      " Speaker0039       1.00      1.00      1.00         5\n",
      " Speaker0040       1.00      1.00      1.00         3\n",
      " Speaker0041       1.00      1.00      1.00         3\n",
      " Speaker0042       1.00      1.00      1.00         4\n",
      " Speaker0043       0.60      1.00      0.75         3\n",
      " Speaker0044       1.00      1.00      1.00         3\n",
      " Speaker0045       1.00      1.00      1.00         4\n",
      " Speaker0046       1.00      1.00      1.00         3\n",
      " Speaker0047       1.00      1.00      1.00         3\n",
      " Speaker0048       1.00      1.00      1.00         3\n",
      " Speaker0049       1.00      1.00      1.00         5\n",
      " Speaker0050       1.00      1.00      1.00         3\n",
      "Speaker_0000       1.00      1.00      1.00         9\n",
      "Speaker_0001       1.00      1.00      1.00        11\n",
      "Speaker_0002       0.92      1.00      0.96        12\n",
      "Speaker_0003       1.00      1.00      1.00         8\n",
      "Speaker_0004       1.00      1.00      1.00         9\n",
      "Speaker_0005       1.00      1.00      1.00         7\n",
      "Speaker_0006       1.00      0.86      0.92         7\n",
      "Speaker_0007       1.00      1.00      1.00         6\n",
      "Speaker_0008       1.00      1.00      1.00         9\n",
      "Speaker_0009       1.00      0.83      0.91         6\n",
      "Speaker_0010       1.00      1.00      1.00         5\n",
      "Speaker_0011       1.00      0.83      0.91         6\n",
      "Speaker_0012       1.00      1.00      1.00         3\n",
      "Speaker_0013       0.75      1.00      0.86         3\n",
      "Speaker_0014       1.00      1.00      1.00         5\n",
      "Speaker_0015       1.00      1.00      1.00         3\n",
      "Speaker_0016       1.00      1.00      1.00         4\n",
      "Speaker_0017       1.00      1.00      1.00         4\n",
      "Speaker_0018       1.00      1.00      1.00         4\n",
      "Speaker_0019       1.00      1.00      1.00         3\n",
      "Speaker_0020       1.00      1.00      1.00         1\n",
      "Speaker_0021       1.00      1.00      1.00         5\n",
      "Speaker_0023       1.00      1.00      1.00         3\n",
      "Speaker_0024       1.00      1.00      1.00         5\n",
      "Speaker_0025       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.98       230\n",
      "   macro avg       0.99      0.98      0.98       230\n",
      "weighted avg       0.99      0.98      0.98       230\n",
      "\n",
      "\n",
      "‚úì Saved model bundle ‚Üí C-models/speaker_id_model.joblib\n",
      "‚úì Reports & confusion matrices ‚Üí D-reports\n"
     ]
    }
   ],
   "source": [
    "# scripts/train_eval_classifier.py\n",
    "import os, csv, pathlib, numpy as np, joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INDEX_CSV = \"B-work/1-dataset-ecapa-embeds/index.csv\"\n",
    "MODEL_OUT = \"C-models/speaker_id_model.joblib\"\n",
    "REPORT_DIR= \"D-reports\"  # adjust if you use a different reports folder\n",
    "\n",
    "pathlib.Path(REPORT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(os.path.dirname(MODEL_OUT)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_split(split_name: str):\n",
    "    X, y = [], []\n",
    "    with open(INDEX_CSV, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "        rdr = csv.DictReader(fh)\n",
    "        for r in rdr:\n",
    "            if r[\"split\"] != split_name:\n",
    "                continue\n",
    "            X.append(np.load(r[\"npy\"]))\n",
    "            y.append(r[\"label\"])\n",
    "    if not X:\n",
    "        raise RuntimeError(f\"No rows for split={split_name}. Did you generate split.csv and index.csv with 'split'?\")\n",
    "    return np.vstack(X), np.array(y)\n",
    "\n",
    "# 1) Load splits\n",
    "Xtr, ytr = load_split(\"train\")\n",
    "Xva, yva = load_split(\"val\")\n",
    "Xte, yte = load_split(\"test\")\n",
    "\n",
    "# 2) LabelEncoder: fit on TRAIN ONLY (avoid leakage)\n",
    "le = LabelEncoder()\n",
    "ytr_i = le.fit_transform(ytr)\n",
    "yva_i = le.transform(yva)   # assumes val/test speakers ‚äÜ train speakers\n",
    "yte_i = le.transform(yte)\n",
    "\n",
    "# 3) Scale features: fit on TRAIN ONLY\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "Xtr_s = scaler.fit_transform(Xtr)\n",
    "Xva_s = scaler.transform(Xva)\n",
    "Xte_s = scaler.transform(Xte)\n",
    "\n",
    "# 4) Train\n",
    "clf = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    class_weight=\"balanced\",  # helpful when per-speaker counts differ\n",
    "    n_jobs=None\n",
    ")\n",
    "clf.fit(Xtr_s, ytr_i)\n",
    "\n",
    "# 5) Evaluate helper\n",
    "def evaluate(Xs, ys_i, split_name):\n",
    "    yhat = clf.predict(Xs)\n",
    "    acc  = accuracy_score(ys_i, yhat)\n",
    "    f1m  = f1_score(ys_i, yhat, average=\"macro\")\n",
    "    rep  = classification_report(ys_i, yhat, target_names=le.classes_)\n",
    "    cm   = confusion_matrix(ys_i, yhat)\n",
    "\n",
    "    # print to console\n",
    "    print(f\"\\n== {split_name.upper()} ==\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Macro-F1: {f1m:.4f}\")\n",
    "    print(rep)\n",
    "\n",
    "    # save text report\n",
    "    with open(os.path.join(REPORT_DIR, f\"{split_name}_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\nMacro-F1: {f1m:.4f}\\n\\n{rep}\")\n",
    "\n",
    "    # save confusion matrix fig\n",
    "    plt.figure(figsize=(max(6, 0.5*len(le.classes_)), max(5, 0.5*len(le.classes_))))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"Confusion Matrix - {split_name}\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.xticks(range(len(le.classes_)), le.classes_, rotation=90)\n",
    "    plt.yticks(range(len(le.classes_)), le.classes_)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(REPORT_DIR, f\"{split_name}_confusion_matrix.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# 6) Run evals\n",
    "evaluate(Xtr_s, ytr_i, \"train\")\n",
    "evaluate(Xva_s, yva_i, \"val\")\n",
    "evaluate(Xte_s, yte_i, \"test\")\n",
    "\n",
    "# 7) Save model bundle\n",
    "joblib.dump(\n",
    "    {\"clf\": clf, \"scaler\": scaler, \"label_encoder\": le},\n",
    "    MODEL_OUT\n",
    ")\n",
    "print(f\"\\n‚úì Saved model bundle ‚Üí {MODEL_OUT}\")\n",
    "print(f\"‚úì Reports & confusion matrices ‚Üí {REPORT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5724f52",
   "metadata": {},
   "source": [
    "### # scripts/predict_one_random_val.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c2b32d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth: Speaker0029\n",
      "Pred : Speaker0029\n",
      "\n",
      "Top-3:\n",
      "  Speaker0029: 0.998\n",
      "  Speaker0032: 0.000\n",
      "  Speaker0039: 0.000\n"
     ]
    }
   ],
   "source": [
    "# scripts/predict_one_random_val.py\n",
    "import csv, random, numpy as np, joblib\n",
    "\n",
    "INDEX   = \"B-work/1-dataset-ecapa-embeds/index.csv\"\n",
    "MODEL   = \"C-models/speaker_id_model.joblib\"\n",
    "\n",
    "# 1) Load model bundle\n",
    "bundle = joblib.load(MODEL)\n",
    "clf    = bundle[\"clf\"]\n",
    "scaler = bundle[\"scaler\"]\n",
    "le     = bundle[\"label_encoder\"]\n",
    "\n",
    "# 2) Collect only VAL rows\n",
    "val_rows = []\n",
    "with open(INDEX, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    for r in csv.DictReader(fh):\n",
    "        if r.get(\"split\") == \"val\":\n",
    "            val_rows.append(r)\n",
    "\n",
    "if not val_rows:\n",
    "    raise RuntimeError(\"No 'val' rows found. Did you generate split.csv and index.csv with a val split?\")\n",
    "\n",
    "# 3) Pick ONE at random (set seed for repeatability if you want)\n",
    "random.seed()  # or random.seed(42)\n",
    "row = random.choice(val_rows)\n",
    "\n",
    "# 4) Load embedding, scale, predict\n",
    "emb   = np.load(row[\"npy\"]).reshape(1, -1)\n",
    "emb_s = scaler.transform(emb)\n",
    "pred_i = clf.predict(emb_s)[0]\n",
    "pred   = le.inverse_transform([pred_i])[0]\n",
    "\n",
    "print(f\"Truth: {row['label']}\")\n",
    "print(f\"Pred : {pred}\")\n",
    "\n",
    "# Optional: show top-3 if classifier supports probabilities\n",
    "if hasattr(clf, \"predict_proba\"):\n",
    "    probs = clf.predict_proba(emb_s)[0]\n",
    "    topk  = np.argsort(probs)[::-1][:3]\n",
    "    print(\"\\nTop-3:\")\n",
    "    for i in topk:\n",
    "        print(f\"  {le.classes_[i]}: {probs[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d427f",
   "metadata": {},
   "source": [
    "### # üì¶ Cell 1 ‚Äî Setup & helpers (paths, loaders, plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "306ba7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & helpers ===\n",
    "import os, csv, pathlib, numpy as np, joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, Normalizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Your project paths\n",
    "INDEX_CSV  = \"B-work/1-dataset-ecapa-embeds/index.csv\"\n",
    "REPORT_DIR = \"D-reports/comparison\"\n",
    "MODEL_DIR  = \"C-models/comparison\"\n",
    "\n",
    "pathlib.Path(REPORT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_split(index_csv, split):\n",
    "    X, y = [], []\n",
    "    with open(index_csv, newline=\"\", encoding=\"utf-8\") as fh:\n",
    "        rdr = csv.DictReader(fh)\n",
    "        for r in rdr:\n",
    "            if r[\"split\"] != split:\n",
    "                continue\n",
    "            X.append(np.load(r[\"npy\"]))\n",
    "            y.append(r[\"label\"])\n",
    "    if not X:\n",
    "        raise RuntimeError(f\"No rows for split={split}. Regenerate index.csv with split?\")\n",
    "    return np.vstack(X), np.array(y)\n",
    "\n",
    "def save_cm(cm, classes, out_png, title):\n",
    "    plt.figure(figsize=(max(6, 0.5*len(classes)), max(5, 0.5*len(classes))))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.xticks(range(len(classes)), classes, rotation=90)\n",
    "    plt.yticks(range(len(classes)), classes)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d6f25",
   "metadata": {},
   "source": [
    "### üì• Cell 2 ‚Äî Load data & fit preprocessors (train-only fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be69e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: \n",
      "  train: (2051, 192) \n",
      "  val  : (230, 192) \n",
      "  test : (230, 192) \n",
      "  classes: 50\n"
     ]
    }
   ],
   "source": [
    "# === Load splits ===\n",
    "Xtr, ytr = load_split(INDEX_CSV, \"train\")\n",
    "Xva, yva = load_split(INDEX_CSV, \"val\")\n",
    "Xte, yte = load_split(INDEX_CSV, \"test\")\n",
    "\n",
    "# === Label encoder: fit on TRAIN only ===\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "ytr_i = le.fit_transform(ytr)\n",
    "yva_i = le.transform(yva)\n",
    "yte_i = le.transform(yte)\n",
    "\n",
    "# === Feature scaling: fit on TRAIN only ===\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "Xtr_s  = scaler.fit_transform(Xtr)\n",
    "Xva_s  = scaler.transform(Xva)\n",
    "Xte_s  = scaler.transform(Xte)\n",
    "\n",
    "print(\"Data ready:\",\n",
    "      \"\\n  train:\", Xtr_s.shape,\n",
    "      \"\\n  val  :\", Xva_s.shape,\n",
    "      \"\\n  test :\", Xte_s.shape,\n",
    "      \"\\n  classes:\", len(le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc2529",
   "metadata": {},
   "source": [
    "### üß™ Cell 3 ‚Äî Evaluate helper (reused by all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae70d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation helper used by all models ===\n",
    "def evaluate_and_report(name, clf, Xtr_s, ytr_i, Xva_s, yva_i, Xte_s, yte_i, save_bundle=True):\n",
    "    clf.fit(Xtr_s, ytr_i)\n",
    "\n",
    "    def do_eval(Xs, ys_i, split):\n",
    "        yhat = clf.predict(Xs)\n",
    "        acc  = accuracy_score(ys_i, yhat)\n",
    "        f1m  = f1_score(ys_i, yhat, average=\"macro\")\n",
    "        rep  = classification_report(ys_i, yhat, target_names=le.classes_)\n",
    "        cm   = confusion_matrix(ys_i, yhat)\n",
    "\n",
    "        # save report & confusion matrix\n",
    "        prefix = f\"{name}_{split}\"\n",
    "        with open(os.path.join(REPORT_DIR, f\"{prefix}_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Accuracy: {acc:.4f}\\nMacro-F1: {f1m:.4f}\\n\\n{rep}\")\n",
    "        save_cm(cm, le.classes_, os.path.join(REPORT_DIR, f\"{prefix}_cm.png\"),\n",
    "                f\"{name} ‚Äì Confusion Matrix ({split})\")\n",
    "\n",
    "        print(f\"\\n== {name} :: {split.upper()} ==\")\n",
    "        print(f\"Accuracy: {acc:.4f} | Macro-F1: {f1m:.4f}\")\n",
    "        return acc, f1m\n",
    "\n",
    "    acc_tr, f1_tr = do_eval(Xtr_s, ytr_i, \"train\")\n",
    "    acc_va, f1_va = do_eval(Xva_s, yva_i, \"val\")\n",
    "    acc_te, f1_te = do_eval(Xte_s, yte_i, \"test\")\n",
    "\n",
    "    # optional: save model bundle\n",
    "    if save_bundle:\n",
    "        joblib.dump(\n",
    "            {\"clf\": clf, \"scaler\": scaler, \"label_encoder\": le},\n",
    "            os.path.join(MODEL_DIR, f\"{name}.joblib\")\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"acc_train\": acc_tr, \"f1_train\": f1_tr,\n",
    "        \"acc_val\": acc_va,   \"f1_val\": f1_va,\n",
    "        \"acc_test\": acc_te,  \"f1_test\": f1_te\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c54df",
   "metadata": {},
   "source": [
    "#### üìà Cell 4 ‚Äî (Type 1: default) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62760d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== logreg :: TRAIN ==\n",
      "Accuracy: 0.9990 | Macro-F1: 0.9991\n",
      "\n",
      "== logreg :: VAL ==\n",
      "Accuracy: 0.9913 | Macro-F1: 0.9958\n",
      "\n",
      "== logreg :: TEST ==\n",
      "Accuracy: 0.9826 | Macro-F1: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'logreg',\n",
       " 'acc_train': 0.9990248659190639,\n",
       " 'f1_train': 0.9991243025566553,\n",
       " 'acc_val': 0.991304347826087,\n",
       " 'f1_val': 0.9958235294117647,\n",
       " 'acc_test': 0.9826086956521739,\n",
       " 'f1_test': 0.9821680319680319}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=4000, class_weight=\"balanced\", n_jobs=None)\n",
    "res_logreg = evaluate_and_report(\"logreg\", logreg, Xtr_s, ytr_i, Xva_s, yva_i, Xte_s, yte_i)\n",
    "res_logreg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4f8b3",
   "metadata": {},
   "source": [
    "#### üìà Cell 5 ‚Äî (Type 2) Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce367f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== linear_svc :: TRAIN ==\n",
      "Accuracy: 0.9990 | Macro-F1: 0.9991\n",
      "\n",
      "== linear_svc :: VAL ==\n",
      "Accuracy: 0.9957 | Macro-F1: 0.9975\n",
      "\n",
      "== linear_svc :: TEST ==\n",
      "Accuracy: 0.9739 | Macro-F1: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'linear_svc',\n",
       " 'acc_train': 0.9990248659190639,\n",
       " 'f1_train': 0.9991243025566553,\n",
       " 'acc_val': 0.9956521739130435,\n",
       " 'f1_val': 0.9974901960784314,\n",
       " 'acc_test': 0.9739130434782609,\n",
       " 'f1_test': 0.9751621711621712}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC(class_weight=\"balanced\")\n",
    "res_lsvc = evaluate_and_report(\"linear_svc\", lsvc, Xtr_s, ytr_i, Xva_s, yva_i, Xte_s, yte_i)\n",
    "res_lsvc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225d65f",
   "metadata": {},
   "source": [
    "#### üìà Cell 6 ‚Äî (Type 3) kNN (optionally with L2 normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd02e320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== knn_k5_l2 :: TRAIN ==\n",
      "Accuracy: 1.0000 | Macro-F1: 1.0000\n",
      "\n",
      "== knn_k5_l2 :: VAL ==\n",
      "Accuracy: 0.9870 | Macro-F1: 0.9935\n",
      "\n",
      "== knn_k5_l2 :: TEST ==\n",
      "Accuracy: 0.9696 | Macro-F1: 0.9665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'knn_k5_l2',\n",
       " 'acc_train': 1.0,\n",
       " 'f1_train': 1.0,\n",
       " 'acc_val': 0.9869565217391304,\n",
       " 'f1_val': 0.9934527160146357,\n",
       " 'acc_test': 0.9695652173913043,\n",
       " 'f1_test': 0.9665051439788281}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Toggle these if you want to experiment:\n",
    "USE_L2_NORMALIZE = True   # often helps kNN on embedding spaces\n",
    "K_FOR_KNN        = 5      # try 3, 5, 7\n",
    "\n",
    "if USE_L2_NORMALIZE:\n",
    "    l2 = Normalizer(norm=\"l2\")\n",
    "    Xtr_knn = l2.fit_transform(Xtr_s)\n",
    "    Xva_knn = l2.transform(Xva_s)\n",
    "    Xte_knn = l2.transform(Xte_s)\n",
    "else:\n",
    "    Xtr_knn, Xva_knn, Xte_knn = Xtr_s, Xva_s, Xte_s\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=K_FOR_KNN, weights=\"distance\")\n",
    "res_knn = evaluate_and_report(f\"knn_k{K_FOR_KNN}{'_l2' if USE_L2_NORMALIZE else ''}\",\n",
    "                              knn, Xtr_knn, ytr_i, Xva_knn, yva_i, Xte_knn, yte_i)\n",
    "res_knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f099d",
   "metadata": {},
   "source": [
    "### üßæ Cell 7 ‚Äî Summary table across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f73230c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>acc_val</th>\n",
       "      <th>f1_val</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>0.999124</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>0.995824</td>\n",
       "      <td>0.982609</td>\n",
       "      <td>0.982168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear_svc</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>0.999124</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.997490</td>\n",
       "      <td>0.973913</td>\n",
       "      <td>0.975162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knn_k5_l2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986957</td>\n",
       "      <td>0.993453</td>\n",
       "      <td>0.969565</td>\n",
       "      <td>0.966505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  acc_train  f1_train   acc_val    f1_val  acc_test   f1_test\n",
       "0      logreg   0.999025  0.999124  0.991304  0.995824  0.982609  0.982168\n",
       "1  linear_svc   0.999025  0.999124  0.995652  0.997490  0.973913  0.975162\n",
       "2   knn_k5_l2   1.000000  1.000000  0.986957  0.993453  0.969565  0.966505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Summary saved ‚Üí D-reports/comparison\\summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([res_logreg, res_lsvc, res_knn])\n",
    "display(summary)\n",
    "\n",
    "# Save for your PPT/report\n",
    "out_csv = os.path.join(REPORT_DIR, \"summary.csv\")\n",
    "summary.to_csv(out_csv, index=False)\n",
    "print(\"‚úì Summary saved ‚Üí\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9454a1",
   "metadata": {},
   "source": [
    "# Live Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97f859",
   "metadata": {},
   "source": [
    "## üì• 1a. all sample voices: Normalize enrollment audio + build manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5b2c6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Normalized enrollment audio ‚Üí A-data\\2-processed\\2-1-wav16k\\2-1-2-speaker-enroll\n",
      "‚úì Wrote manifest ‚Üí A-data\\2-processed\\2-2-manifests\\enroll_index.csv\n",
      "People: ['001-Justin-Anderson', '002-Sam-COACH--Cassidy', '003-Charlie-OWNER--Archer', '004-Claire-CS-LEAD--Hope', '005-Paul-FLEET--Mark']\n",
      "Clips processed: 5\n"
     ]
    }
   ],
   "source": [
    "# Normalize A-data/1-raw/1-2-speaker-enroll --> A-data/2-processed/2-1-wav16k/2-1-2-speaker-enroll\n",
    "# And create A-data/2-processed/2-2-manifests/enroll_index.csv\n",
    "\n",
    "import os, uuid, pathlib, csv\n",
    "import librosa, soundfile as sf\n",
    "\n",
    "RAW_ENROLL       = pathlib.Path(\"A-data/1-raw/1-2-speaker-enroll\")\n",
    "PROC_ENROLL      = pathlib.Path(\"A-data/2-processed/2-1-wav16k/2-1-2-speaker-enroll\")\n",
    "MANIFESTS_DIR    = pathlib.Path(\"A-data/2-processed/2-2-manifests\")\n",
    "ENROLL_MANIFEST  = MANIFESTS_DIR / \"enroll_index.csv\"\n",
    "\n",
    "PROC_ENROLL.mkdir(parents=True, exist_ok=True)\n",
    "MANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_wav16k(src_path: pathlib.Path, dst_path: pathlib.Path, sr=16000):\n",
    "    # librosa handles wav/flac/mp3/m4a/ogg via audioread\n",
    "    y, _ = librosa.load(str(src_path), sr=sr, mono=True)\n",
    "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sf.write(str(dst_path), y, sr, subtype=\"PCM_16\")\n",
    "\n",
    "rows = []\n",
    "count_files = 0\n",
    "people = []\n",
    "\n",
    "valid_ext = {\".wav\", \".flac\", \".mp3\", \".m4a\", \".ogg\"}\n",
    "\n",
    "for person_dir in sorted(RAW_ENROLL.glob(\"*\")):\n",
    "    if not person_dir.is_dir():\n",
    "        continue\n",
    "    person = person_dir.name\n",
    "    people.append(person)\n",
    "    out_person_dir = PROC_ENROLL / person\n",
    "\n",
    "    for src in person_dir.rglob(\"*\"):\n",
    "        if src.suffix.lower() not in valid_ext:\n",
    "            continue\n",
    "        # create anonymized, collision-proof filename (keep stems if you prefer)\n",
    "        dst = out_person_dir / f\"{src.stem}.wav\"\n",
    "        try:\n",
    "            write_wav16k(src, dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped {src} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # duration for manifest (use librosa on the written file)\n",
    "        dur = librosa.get_duration(path=str(dst))\n",
    "        rows.append({\"path\": str(dst), \"person\": person, \"duration_s\": f\"{dur:.2f}\"})\n",
    "        count_files += 1\n",
    "\n",
    "# write manifest\n",
    "with open(ENROLL_MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.DictWriter(fh, fieldnames=[\"path\",\"person\",\"duration_s\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "print(\"‚úì Normalized enrollment audio ‚Üí\", PROC_ENROLL)\n",
    "print(\"‚úì Wrote manifest ‚Üí\", ENROLL_MANIFEST)\n",
    "print(f\"People: {people}\")\n",
    "print(f\"Clips processed: {count_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60a981",
   "metadata": {},
   "source": [
    "## üß† 1b. Sample Voices: Build ECAPA mean embeddings per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b64d94fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì 001-Justin-Anderson: 1 clips ‚Üí mean embedding computed\n",
      "‚úì 002-Sam-COACH--Cassidy: 1 clips ‚Üí mean embedding computed\n",
      "‚úì 003-Charlie-OWNER--Archer: 1 clips ‚Üí mean embedding computed\n",
      "‚úì 004-Claire-CS-LEAD--Hope: 1 clips ‚Üí mean embedding computed\n",
      "‚úì 005-Paul-FLEET--Mark: 1 clips ‚Üí mean embedding computed\n",
      "‚úì Saved enrollment means ‚Üí B-work\\2-speaker-enroll-ecapa\\ecapa_means.json\n"
     ]
    }
   ],
   "source": [
    "# Build per-person ECAPA mean embeddings from processed enrollment WAVs\n",
    "# Uses your local model cache at B-work/0-ecapa-model-cache\n",
    "\n",
    "import json, numpy as np, torch, torchaudio, pathlib\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "PROC_ENROLL     = pathlib.Path(\"A-data/2-processed/2-1-wav16k/2-1-2-speaker-enroll\")\n",
    "LOCAL_MODEL_DIR = pathlib.Path(\"B-work/0-ecapa-model-cache\")  # your local ECAPA snapshot folder\n",
    "OUT_JSON        = pathlib.Path(\"B-work/2-speaker-enroll-ecapa/ecapa_means.json\")\n",
    "\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "enc = EncoderClassifier.from_hparams(\n",
    "    source=str(LOCAL_MODEL_DIR),  # load from local cache, no symlinks\n",
    "    savedir=str(LOCAL_MODEL_DIR),\n",
    "    run_opts={\"device\": device},\n",
    ")\n",
    "\n",
    "def load_mono16k_torch(path: str):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    return wav\n",
    "\n",
    "means = {}\n",
    "for person_dir in sorted(PROC_ENROLL.glob(\"*\")):\n",
    "    if not person_dir.is_dir():\n",
    "        continue\n",
    "    person = person_dir.name\n",
    "    embs = []\n",
    "    for wav_path in person_dir.glob(\"*.wav\"):\n",
    "        wav = load_mono16k_torch(str(wav_path)).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = enc.encode_batch(wav).squeeze().cpu().numpy()\n",
    "        embs.append(emb)\n",
    "    if embs:\n",
    "        means[person] = np.mean(np.stack(embs), axis=0).tolist()\n",
    "        print(f\"‚úì {person}: {len(embs)} clips ‚Üí mean embedding computed\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {person}: no WAVs found in {person_dir}\")\n",
    "\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(means, fh, indent=2)\n",
    "\n",
    "print(\"‚úì Saved enrollment means ‚Üí\", OUT_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5583dd",
   "metadata": {},
   "source": [
    "## X 2a. Normalize meeting audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daa3a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Normalized meeting audio ‚Üí A-data\\2-processed\\2-1-wav16k\\2-1-3-client-meetings\n",
      "‚úì Wrote manifest ‚Üí A-data\\2-processed\\2-2-manifests\\meetings_index.csv\n",
      "Meetings: ['2025-09-08-Fake-Meeting-01']\n",
      "Clips processed: 1\n"
     ]
    }
   ],
   "source": [
    "# Normalize A-data/1-raw/1-3-client-meetings --> A-data/2-processed/2-1-wav16k/2-1-3-client-meetings\n",
    "# And create A-data/2-processed/2-2-manifests/meetings_index.csv\n",
    "\n",
    "import os, pathlib, csv\n",
    "import librosa, soundfile as sf\n",
    "\n",
    "RAW_MEETINGS    = pathlib.Path(\"A-data/1-raw/1-3-client-meetings\")\n",
    "PROC_MEETINGS   = pathlib.Path(\"A-data/2-processed/2-1-wav16k/2-1-3-client-meetings\")\n",
    "MANIFESTS_DIR   = pathlib.Path(\"A-data/2-processed/2-2-manifests\")\n",
    "MEETING_MANIFEST= MANIFESTS_DIR / \"meetings_index.csv\"\n",
    "\n",
    "PROC_MEETINGS.mkdir(parents=True, exist_ok=True)\n",
    "MANIFESTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_wav16k(src_path: pathlib.Path, dst_path: pathlib.Path, sr=16000):\n",
    "    y, _ = librosa.load(str(src_path), sr=sr, mono=True)\n",
    "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sf.write(str(dst_path), y, sr, subtype=\"PCM_16\")\n",
    "\n",
    "rows, count_files, meetings = [], 0, []\n",
    "\n",
    "valid_ext = {\".wav\", \".flac\", \".mp3\", \".m4a\", \".ogg\"}\n",
    "\n",
    "for meeting_dir in sorted(RAW_MEETINGS.glob(\"*\")):\n",
    "    if not meeting_dir.is_dir():\n",
    "        continue\n",
    "    meeting_id = meeting_dir.name\n",
    "    meetings.append(meeting_id)\n",
    "    out_meeting_dir = PROC_MEETINGS / meeting_id\n",
    "\n",
    "    for src in meeting_dir.rglob(\"*\"):\n",
    "        if src.suffix.lower() not in valid_ext:\n",
    "            continue\n",
    "        dst = out_meeting_dir / f\"{src.stem}.wav\"\n",
    "        try:\n",
    "            write_wav16k(src, dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped {src} due to error: {e}\")\n",
    "            continue\n",
    "        dur = librosa.get_duration(path=str(dst))\n",
    "        rows.append({\"path\": str(dst), \"meeting_id\": meeting_id, \"duration_s\": f\"{dur:.2f}\"})\n",
    "        count_files += 1\n",
    "\n",
    "with open(MEETING_MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "    w = csv.DictWriter(fh, fieldnames=[\"path\",\"meeting_id\",\"duration_s\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "print(\"‚úì Normalized meeting audio ‚Üí\", PROC_MEETINGS)\n",
    "print(\"‚úì Wrote manifest ‚Üí\", MEETING_MANIFEST)\n",
    "print(f\"Meetings: {meetings}\")\n",
    "print(f\"Clips processed: {count_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06639bbc",
   "metadata": {},
   "source": [
    "## üß≠ 2b. (pre-Speaker Attempt) Diarization + STT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7e0a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\utils\\reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Diarized: ElevenLabs_Fake_Meeting_01.wav ‚Üí B-work\\3-client-meetings-diarization\\2025-09-08-Fake-Meeting-01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.38</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.98</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.80</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.39</td>\n",
       "      <td>7.38</td>\n",
       "      <td>2.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.75</td>\n",
       "      <td>9.01</td>\n",
       "      <td>1.26</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.31</td>\n",
       "      <td>10.55</td>\n",
       "      <td>1.24</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>166.75</td>\n",
       "      <td>168.16</td>\n",
       "      <td>1.41</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>168.63</td>\n",
       "      <td>169.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>170.57</td>\n",
       "      <td>171.59</td>\n",
       "      <td>1.02</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>171.99</td>\n",
       "      <td>172.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>172.67</td>\n",
       "      <td>173.52</td>\n",
       "      <td>0.85</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start     end  duration     cluster\n",
       "0     0.01    1.38      1.38  SPEAKER_02\n",
       "1     1.98    3.78      1.80  SPEAKER_02\n",
       "2     4.39    7.38      2.99  SPEAKER_00\n",
       "3     7.75    9.01      1.26  SPEAKER_00\n",
       "4     9.31   10.55      1.24  SPEAKER_00\n",
       "..     ...     ...       ...         ...\n",
       "72  166.75  168.16      1.41  SPEAKER_02\n",
       "73  168.63  169.48      0.85  SPEAKER_02\n",
       "74  170.57  171.59      1.02  SPEAKER_00\n",
       "75  171.99  172.27      0.27  SPEAKER_00\n",
       "76  172.67  173.52      0.85  SPEAKER_02\n",
       "\n",
       "[77 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Diarize all meetings under:\n",
    "#   A-data/2-processed/2-1-wav16k/2-1-3-client-meetings/<meeting_id>/*.wav\n",
    "# Save outputs to:\n",
    "#   B-work/3-client-meetings-diarization/<meeting_id>/{diarization.rttm, segments.(tsv|json)}\n",
    "\n",
    "import os, json, pathlib\n",
    "from dotenv import load_dotenv\n",
    "from pyannote.audio import Pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "MEET_ROOT = pathlib.Path(\"A-data/2-processed/2-1-wav16k/2-1-3-client-meetings\")\n",
    "OUT_ROOT  = pathlib.Path(\"B-work/3-client-meetings-diarization\")\n",
    "\n",
    "load_dotenv()\n",
    "import os as _os\n",
    "hf_token = _os.getenv(\"HF_TOKEN\")\n",
    "assert hf_token, \"Add HF_TOKEN=... to your .env\"\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=hf_token,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def diarize_file(wav_path: pathlib.Path, out_dir: pathlib.Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    diar = pipeline(str(wav_path))\n",
    "\n",
    "    # Save RTTM (standard who-spoke-when)\n",
    "    with (out_dir / \"diarization.rttm\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        diar.write_rttm(f)\n",
    "\n",
    "    # Save segments as TSV + JSON\n",
    "    seg_rows = []\n",
    "    for seg, _, label in diar.itertracks(yield_label=True):\n",
    "        seg_rows.append({\n",
    "            \"start\": round(seg.start, 2),\n",
    "            \"end\": round(seg.end, 2),\n",
    "            \"duration\": round(seg.end - seg.start, 2),\n",
    "            \"cluster\": label,\n",
    "        })\n",
    "\n",
    "    with (out_dir / \"segments.tsv\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"start\\tend\\tduration\\tcluster\\n\")\n",
    "        for r in seg_rows:\n",
    "            f.write(f\"{r['start']}\\t{r['end']}\\t{r['duration']}\\t{r['cluster']}\\n\")\n",
    "\n",
    "    (out_dir / \"segments.json\").write_text(json.dumps(seg_rows, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"‚úì Diarized: {wav_path.name} ‚Üí {out_dir}\")\n",
    "    df = pd.DataFrame(seg_rows)\n",
    "    return df\n",
    "\n",
    "# Process each meeting folder (first .wav inside each)\n",
    "for meeting_dir in sorted(MEET_ROOT.glob(\"*\")):\n",
    "    if not meeting_dir.is_dir():\n",
    "        continue\n",
    "    wavs = sorted(meeting_dir.glob(\"*.wav\"))\n",
    "    if not wavs:\n",
    "        print(f\"‚ö†Ô∏è No .wav in {meeting_dir}\")\n",
    "        continue\n",
    "    df = diarize_file(wavs[0], OUT_ROOT / meeting_dir.name)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319cf743",
   "metadata": {},
   "source": [
    "## üß≠ 2c. (pre-Speaker Attempt) Diarization + STT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f4476ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CodeProjs\\speaker-id-demo\\.venv\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Named segments ‚Üí B-work\\4-client-meetings-named-diary\\2025-09-08-Fake-Meeting-01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>cluster</th>\n",
       "      <th>who</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.37</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.98</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.80</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.39</td>\n",
       "      <td>7.38</td>\n",
       "      <td>2.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.75</td>\n",
       "      <td>9.01</td>\n",
       "      <td>1.26</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.31</td>\n",
       "      <td>10.55</td>\n",
       "      <td>1.24</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>166.75</td>\n",
       "      <td>168.16</td>\n",
       "      <td>1.41</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>0.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>168.63</td>\n",
       "      <td>169.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>170.57</td>\n",
       "      <td>171.59</td>\n",
       "      <td>1.02</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>171.99</td>\n",
       "      <td>172.27</td>\n",
       "      <td>0.28</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>172.67</td>\n",
       "      <td>173.52</td>\n",
       "      <td>0.85</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start     end  duration     cluster                        who  \\\n",
       "0     0.01    1.38      1.37  SPEAKER_02     002-Sam-COACH--Cassidy   \n",
       "1     1.98    3.78      1.80  SPEAKER_02     002-Sam-COACH--Cassidy   \n",
       "2     4.39    7.38      2.99  SPEAKER_00  003-Charlie-OWNER--Archer   \n",
       "3     7.75    9.01      1.26  SPEAKER_00  003-Charlie-OWNER--Archer   \n",
       "4     9.31   10.55      1.24  SPEAKER_00  003-Charlie-OWNER--Archer   \n",
       "..     ...     ...       ...         ...                        ...   \n",
       "69  166.75  168.16      1.41  SPEAKER_02     002-Sam-COACH--Cassidy   \n",
       "70  168.63  169.48      0.85  SPEAKER_02                    Unknown   \n",
       "71  170.57  171.59      1.02  SPEAKER_00                    Unknown   \n",
       "72  171.99  172.27      0.28  SPEAKER_00                    Unknown   \n",
       "73  172.67  173.52      0.85  SPEAKER_02                    Unknown   \n",
       "\n",
       "    cosine_sim  \n",
       "0        0.765  \n",
       "1        0.686  \n",
       "2        0.757  \n",
       "3        0.747  \n",
       "4        0.677  \n",
       "..         ...  \n",
       "69       0.728  \n",
       "70       0.572  \n",
       "71       0.567  \n",
       "72       0.217  \n",
       "73       0.522  \n",
       "\n",
       "[74 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Name diarized speakers using enrollment ECAPA means\n",
    "# Outputs:\n",
    "#   B-work/3-client-meetings-diarization-named/<meeting_id>/named_segments.(json|tsv)\n",
    "# Returns:\n",
    "#   A Pandas DataFrame (df_named) for the last meeting processed\n",
    "\n",
    "import os, json, pathlib, numpy as np, torch, torchaudio, pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cdist\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "ENROLL_MEANS = pathlib.Path(\"B-work/2-speaker-enroll-ecapa/ecapa_means.json\")\n",
    "MODEL_DIR    = pathlib.Path(\"B-work/0-ecapa-model-cache\")\n",
    "IN_ROOT      = pathlib.Path(\"B-work/3-client-meetings-diarization\")\n",
    "OUT_ROOT     = pathlib.Path(\"B-work/4-client-meetings-named-diary\")\n",
    "AUDIO_ROOT   = pathlib.Path(\"A-data/2-processed/2-1-wav16k/2-1-3-client-meetings\")\n",
    "\n",
    "assert ENROLL_MEANS.exists(), \"Missing enrollment means JSON. Run the enrollment mean step first.\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "enc = EncoderClassifier.from_hparams(\n",
    "    source=str(MODEL_DIR),\n",
    "    savedir=str(MODEL_DIR),\n",
    "    run_opts={\"device\": device},\n",
    ")\n",
    "\n",
    "def load_mono16k(path: str):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "    if sr != 16000:\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "    return wav, 16000\n",
    "\n",
    "# Enrollment means ‚Üí L2-normalized\n",
    "enroll = json.loads(ENROLL_MEANS.read_text(encoding=\"utf-8\"))\n",
    "E_keys = list(enroll.keys())\n",
    "E_mat  = np.stack([normalize(np.array(enroll[k]).reshape(1, -1))[0] for k in E_keys])\n",
    "\n",
    "COSINE_SIM_THRESHOLD = 0.65  # tune later\n",
    "\n",
    "df_named = None  # will hold last meeting's DataFrame\n",
    "\n",
    "for meeting_dir in sorted(IN_ROOT.glob(\"*\")):\n",
    "    if not meeting_dir.is_dir():\n",
    "        continue\n",
    "    meeting_id = meeting_dir.name\n",
    "    out_dir = OUT_ROOT / meeting_id\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load audio\n",
    "    wavs = list((AUDIO_ROOT/meeting_id).glob(\"*.wav\"))\n",
    "    if not wavs:\n",
    "        print(f\"‚ö†Ô∏è No audio for {meeting_id}\")\n",
    "        continue\n",
    "    wav_path = wavs[0]\n",
    "    wav, sr = load_mono16k(str(wav_path))\n",
    "\n",
    "    # Load diarization segments\n",
    "    segs_path = meeting_dir / \"segments.json\"\n",
    "    if not segs_path.exists():\n",
    "        print(f\"‚ö†Ô∏è No segments.json in {meeting_dir}\")\n",
    "        continue\n",
    "    segs = json.loads(segs_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    # Name each segment\n",
    "    named = []\n",
    "    for s in segs:\n",
    "        s0, s1 = s[\"start\"], s[\"end\"]\n",
    "        if s1 <= s0:\n",
    "            continue\n",
    "        seg_wav = wav[:, int(s0*sr):int(s1*sr)]\n",
    "        if seg_wav.shape[-1] < int(0.2*sr):\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            emb = enc.encode_batch(seg_wav.to(device)).squeeze().cpu().numpy()\n",
    "        emb_n = normalize(emb.reshape(1, -1))\n",
    "        dists = cdist(emb_n, E_mat, metric=\"cosine\")[0]\n",
    "        best_idx = int(np.argmin(dists))\n",
    "        sim = float(1 - dists[best_idx])\n",
    "        who = E_keys[best_idx] if sim >= COSINE_SIM_THRESHOLD else \"Unknown\"\n",
    "        named.append({\n",
    "            \"start\": round(s0, 2),\n",
    "            \"end\": round(s1, 2),\n",
    "            \"duration\": round(s1 - s0, 2),\n",
    "            \"cluster\": s[\"cluster\"],\n",
    "            \"who\": who,\n",
    "            \"cosine_sim\": round(sim, 3),\n",
    "        })\n",
    "\n",
    "    # Save outputs\n",
    "    (out_dir / \"named_segments.json\").write_text(json.dumps(named, indent=2), encoding=\"utf-8\")\n",
    "    df_named = pd.DataFrame(named)\n",
    "    df_named.to_csv(out_dir / \"named_segments.tsv\", sep=\"\\t\", index=False)\n",
    "    print(f\"‚úì Named segments ‚Üí {out_dir}\")\n",
    "\n",
    "# üëâ df_named now contains the DataFrame of the last meeting processed\n",
    "display(df_named)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2cc0d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85.695000</td>\n",
       "      <td>87.527973</td>\n",
       "      <td>1.832973</td>\n",
       "      <td>0.695297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>53.343536</td>\n",
       "      <td>53.315491</td>\n",
       "      <td>0.923005</td>\n",
       "      <td>0.133006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>35.752500</td>\n",
       "      <td>37.885000</td>\n",
       "      <td>1.192500</td>\n",
       "      <td>0.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>85.605000</td>\n",
       "      <td>87.540000</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>0.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>130.365000</td>\n",
       "      <td>132.075000</td>\n",
       "      <td>2.452500</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172.670000</td>\n",
       "      <td>173.520000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            start         end   duration  cosine_sim\n",
       "count   74.000000   74.000000  74.000000   74.000000\n",
       "mean    85.695000   87.527973   1.832973    0.695297\n",
       "std     53.343536   53.315491   0.923005    0.133006\n",
       "min      0.010000    1.380000   0.280000    0.217000\n",
       "25%     35.752500   37.885000   1.192500    0.631000\n",
       "50%     85.605000   87.540000   1.610000    0.747500\n",
       "75%    130.365000  132.075000   2.452500    0.784000\n",
       "max    172.670000  173.520000   4.630000    0.872000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_named.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d11e26",
   "metadata": {},
   "source": [
    "### X 2d. Review Speaker Attempt (grouped with 1 speaker per-row, gap collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9b86075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>cluster</th>\n",
       "      <th>who</th>\n",
       "      <th>cosine_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.37</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.98</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.80</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.39</td>\n",
       "      <td>7.38</td>\n",
       "      <td>2.99</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.75</td>\n",
       "      <td>9.01</td>\n",
       "      <td>1.26</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.31</td>\n",
       "      <td>10.55</td>\n",
       "      <td>1.24</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.98</td>\n",
       "      <td>12.27</td>\n",
       "      <td>1.29</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.91</td>\n",
       "      <td>14.69</td>\n",
       "      <td>1.78</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.07</td>\n",
       "      <td>16.41</td>\n",
       "      <td>1.34</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.97</td>\n",
       "      <td>17.41</td>\n",
       "      <td>0.44</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.89</td>\n",
       "      <td>20.35</td>\n",
       "      <td>2.46</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end  duration     cluster                        who  cosine_sim\n",
       "0   0.01   1.38      1.37  SPEAKER_02     002-Sam-COACH--Cassidy       0.765\n",
       "1   1.98   3.78      1.80  SPEAKER_02     002-Sam-COACH--Cassidy       0.686\n",
       "2   4.39   7.38      2.99  SPEAKER_00  003-Charlie-OWNER--Archer       0.757\n",
       "3   7.75   9.01      1.26  SPEAKER_00  003-Charlie-OWNER--Archer       0.747\n",
       "4   9.31  10.55      1.24  SPEAKER_00  003-Charlie-OWNER--Archer       0.677\n",
       "5  10.98  12.27      1.29  SPEAKER_00  003-Charlie-OWNER--Archer       0.707\n",
       "6  12.91  14.69      1.78  SPEAKER_00                    Unknown       0.616\n",
       "7  15.07  16.41      1.34  SPEAKER_00  003-Charlie-OWNER--Archer       0.703\n",
       "8  16.97  17.41      0.44  SPEAKER_00                    Unknown       0.546\n",
       "9  17.89  20.35      2.46  SPEAKER_00  003-Charlie-OWNER--Archer       0.770"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>talk_duration</th>\n",
       "      <th>span_duration</th>\n",
       "      <th>pauses</th>\n",
       "      <th>gap_total</th>\n",
       "      <th>segments_merged</th>\n",
       "      <th>who</th>\n",
       "      <th>cluster_first</th>\n",
       "      <th>cosine_sim_mean</th>\n",
       "      <th>cosine_sim_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.39</td>\n",
       "      <td>12.27</td>\n",
       "      <td>6.78</td>\n",
       "      <td>7.88</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>4</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.91</td>\n",
       "      <td>14.69</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.07</td>\n",
       "      <td>16.41</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.97</td>\n",
       "      <td>17.41</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.89</td>\n",
       "      <td>20.35</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.76</td>\n",
       "      <td>21.81</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22.47</td>\n",
       "      <td>26.63</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.56</td>\n",
       "      <td>29.09</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29.38</td>\n",
       "      <td>30.30</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31.11</td>\n",
       "      <td>33.06</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33.51</td>\n",
       "      <td>34.63</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35.02</td>\n",
       "      <td>37.63</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>004-Claire-CS-LEAD--Hope</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>37.95</td>\n",
       "      <td>38.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38.85</td>\n",
       "      <td>61.20</td>\n",
       "      <td>20.00</td>\n",
       "      <td>22.35</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>7</td>\n",
       "      <td>004-Claire-CS-LEAD--Hope</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.773143</td>\n",
       "      <td>0.711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>61.69</td>\n",
       "      <td>63.00</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>63.39</td>\n",
       "      <td>65.00</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>65.90</td>\n",
       "      <td>66.66</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>0.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>67.12</td>\n",
       "      <td>68.24</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>68.96</td>\n",
       "      <td>93.32</td>\n",
       "      <td>20.64</td>\n",
       "      <td>24.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.72</td>\n",
       "      <td>9</td>\n",
       "      <td>005-Paul-FLEET--Mark</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>0.781667</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>93.91</td>\n",
       "      <td>101.54</td>\n",
       "      <td>6.54</td>\n",
       "      <td>7.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.821500</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>102.74</td>\n",
       "      <td>105.58</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>105.98</td>\n",
       "      <td>107.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>107.97</td>\n",
       "      <td>109.43</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>109.96</td>\n",
       "      <td>113.81</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>114.59</td>\n",
       "      <td>116.10</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.629000</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>116.54</td>\n",
       "      <td>117.89</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>118.36</td>\n",
       "      <td>119.14</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>119.67</td>\n",
       "      <td>122.06</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>122.66</td>\n",
       "      <td>123.35</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>123.64</td>\n",
       "      <td>128.65</td>\n",
       "      <td>4.52</td>\n",
       "      <td>5.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2</td>\n",
       "      <td>004-Claire-CS-LEAD--Hope</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>129.18</td>\n",
       "      <td>130.38</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>130.76</td>\n",
       "      <td>143.90</td>\n",
       "      <td>11.53</td>\n",
       "      <td>13.14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.61</td>\n",
       "      <td>5</td>\n",
       "      <td>004-Claire-CS-LEAD--Hope</td>\n",
       "      <td>SPEAKER_03</td>\n",
       "      <td>0.786200</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>144.34</td>\n",
       "      <td>155.76</td>\n",
       "      <td>9.20</td>\n",
       "      <td>11.42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>4</td>\n",
       "      <td>005-Paul-FLEET--Mark</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>0.793250</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>156.71</td>\n",
       "      <td>168.16</td>\n",
       "      <td>9.06</td>\n",
       "      <td>11.45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>6</td>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.754333</td>\n",
       "      <td>0.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>168.63</td>\n",
       "      <td>173.52</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.89</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>4</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     start     end  talk_duration  span_duration  pauses  gap_total  \\\n",
       "0     0.01    3.78           3.17           3.77     1.0       0.60   \n",
       "1     4.39   12.27           6.78           7.88     3.0       1.10   \n",
       "2    12.91   14.69           1.78           1.78     0.0       0.00   \n",
       "3    15.07   16.41           1.34           1.34     0.0       0.00   \n",
       "4    16.97   17.41           0.44           0.44     0.0       0.00   \n",
       "5    17.89   20.35           2.46           2.46     0.0       0.00   \n",
       "6    20.76   21.81           1.05           1.05     0.0       0.00   \n",
       "7    22.47   26.63           3.61           4.16     1.0       0.55   \n",
       "8    27.56   29.09           1.53           1.53     0.0       0.00   \n",
       "9    29.38   30.30           0.92           0.92     0.0       0.00   \n",
       "10   31.11   33.06           1.95           1.95     0.0       0.00   \n",
       "11   33.51   34.63           1.12           1.12     0.0       0.00   \n",
       "12   35.02   37.63           2.61           2.61     0.0       0.00   \n",
       "13   37.95   38.65           0.70           0.70     0.0       0.00   \n",
       "14   38.85   61.20          20.00          22.35     6.0       2.35   \n",
       "15   61.69   63.00           1.31           1.31     0.0       0.00   \n",
       "16   63.39   65.00           1.61           1.61     0.0       0.00   \n",
       "17   65.90   66.66           0.76           0.76     0.0       0.00   \n",
       "18   67.12   68.24           1.12           1.12     0.0       0.00   \n",
       "19   68.96   93.32          20.64          24.36     8.0       3.72   \n",
       "20   93.91  101.54           6.54           7.63     1.0       1.09   \n",
       "21  102.74  105.58           2.18           2.84     1.0       0.66   \n",
       "22  105.98  107.46           1.48           1.48     0.0       0.00   \n",
       "23  107.97  109.43           1.46           1.46     0.0       0.00   \n",
       "24  109.96  113.81           3.85           3.85     0.0       0.00   \n",
       "25  114.59  116.10           1.51           1.51     0.0       0.00   \n",
       "26  116.54  117.89           1.35           1.35     0.0       0.00   \n",
       "27  118.36  119.14           0.78           0.78     0.0       0.00   \n",
       "28  119.67  122.06           2.39           2.39     0.0       0.00   \n",
       "29  122.66  123.35           0.69           0.69     0.0       0.00   \n",
       "30  123.64  128.65           4.52           5.01     1.0       0.49   \n",
       "31  129.18  130.38           1.20           1.20     0.0       0.00   \n",
       "32  130.76  143.90          11.53          13.14     4.0       1.61   \n",
       "33  144.34  155.76           9.20          11.42     3.0       2.22   \n",
       "34  156.71  168.16           9.06          11.45     5.0       2.39   \n",
       "35  168.63  173.52           3.00           4.89     3.0       1.89   \n",
       "\n",
       "    segments_merged                        who cluster_first  cosine_sim_mean  \\\n",
       "0                 2     002-Sam-COACH--Cassidy    SPEAKER_02         0.725500   \n",
       "1                 4  003-Charlie-OWNER--Archer    SPEAKER_00         0.722000   \n",
       "2                 1                    Unknown    SPEAKER_00         0.616000   \n",
       "3                 1  003-Charlie-OWNER--Archer    SPEAKER_00         0.703000   \n",
       "4                 1                    Unknown    SPEAKER_00         0.546000   \n",
       "5                 1  003-Charlie-OWNER--Archer    SPEAKER_00         0.770000   \n",
       "6                 1                    Unknown    SPEAKER_00         0.622000   \n",
       "7                 2  003-Charlie-OWNER--Archer    SPEAKER_00         0.691500   \n",
       "8                 1     002-Sam-COACH--Cassidy    SPEAKER_02         0.771000   \n",
       "9                 1                    Unknown    SPEAKER_02         0.603000   \n",
       "10                1     002-Sam-COACH--Cassidy    SPEAKER_02         0.748000   \n",
       "11                2                    Unknown    SPEAKER_02         0.422000   \n",
       "12                1   004-Claire-CS-LEAD--Hope    SPEAKER_03         0.776000   \n",
       "13                1                    Unknown    SPEAKER_03         0.482000   \n",
       "14                7   004-Claire-CS-LEAD--Hope    SPEAKER_03         0.773143   \n",
       "15                1                    Unknown    SPEAKER_03         0.608000   \n",
       "16                1     002-Sam-COACH--Cassidy    SPEAKER_02         0.749000   \n",
       "17                1                    Unknown    SPEAKER_02         0.561000   \n",
       "18                1     002-Sam-COACH--Cassidy    SPEAKER_02         0.662000   \n",
       "19                9       005-Paul-FLEET--Mark    SPEAKER_01         0.781667   \n",
       "20                2     002-Sam-COACH--Cassidy    SPEAKER_02         0.821500   \n",
       "21                2                    Unknown    SPEAKER_00         0.477500   \n",
       "22                1  003-Charlie-OWNER--Archer    SPEAKER_00         0.684000   \n",
       "23                1                    Unknown    SPEAKER_00         0.637000   \n",
       "24                1  003-Charlie-OWNER--Archer    SPEAKER_00         0.799000   \n",
       "25                1                    Unknown    SPEAKER_00         0.629000   \n",
       "26                1  003-Charlie-OWNER--Archer    SPEAKER_00         0.669000   \n",
       "27                1                    Unknown    SPEAKER_00         0.509000   \n",
       "28                1  003-Charlie-OWNER--Archer    SPEAKER_00         0.742000   \n",
       "29                1                    Unknown    SPEAKER_03         0.353000   \n",
       "30                2   004-Claire-CS-LEAD--Hope    SPEAKER_03         0.791000   \n",
       "31                1                    Unknown    SPEAKER_03         0.512000   \n",
       "32                5   004-Claire-CS-LEAD--Hope    SPEAKER_03         0.786200   \n",
       "33                4       005-Paul-FLEET--Mark    SPEAKER_01         0.793250   \n",
       "34                6     002-Sam-COACH--Cassidy    SPEAKER_02         0.754333   \n",
       "35                4                    Unknown    SPEAKER_02         0.469500   \n",
       "\n",
       "    cosine_sim_min  \n",
       "0            0.686  \n",
       "1            0.677  \n",
       "2            0.616  \n",
       "3            0.703  \n",
       "4            0.546  \n",
       "5            0.770  \n",
       "6            0.622  \n",
       "7            0.676  \n",
       "8            0.771  \n",
       "9            0.603  \n",
       "10           0.748  \n",
       "11           0.318  \n",
       "12           0.776  \n",
       "13           0.482  \n",
       "14           0.711  \n",
       "15           0.608  \n",
       "16           0.749  \n",
       "17           0.561  \n",
       "18           0.662  \n",
       "19           0.652  \n",
       "20           0.771  \n",
       "21           0.347  \n",
       "22           0.684  \n",
       "23           0.637  \n",
       "24           0.799  \n",
       "25           0.629  \n",
       "26           0.669  \n",
       "27           0.509  \n",
       "28           0.742  \n",
       "29           0.353  \n",
       "30           0.759  \n",
       "31           0.512  \n",
       "32           0.763  \n",
       "33           0.743  \n",
       "34           0.651  \n",
       "35           0.217  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df_named is the detailed per-segment table you already have\n",
    "# Required cols: start, end, duration, cluster, who, cosine_sim\n",
    "d = df_named.sort_values(\"start\").reset_index(drop=True).copy()\n",
    "\n",
    "# mark boundaries where the speaker changes (run-length encoding by 'who')\n",
    "change = (d[\"who\"] != d[\"who\"].shift(1)).astype(int)\n",
    "group_id = change.cumsum()\n",
    "\n",
    "# helper to compute gap totals and pause counts per run\n",
    "def run_gaps(g):\n",
    "    # gaps only between consecutive rows inside the run\n",
    "    gaps = (g[\"start\"].iloc[1:].to_numpy() - g[\"end\"].iloc[:-1].to_numpy())\n",
    "    gaps = gaps[gaps > 0] if len(gaps) else np.array([])\n",
    "    return pd.Series({\n",
    "        \"pauses\": len(gaps),                 # number of gaps merged\n",
    "        \"gap_total\": float(gaps.sum()) if len(gaps) else 0.0\n",
    "    })\n",
    "\n",
    "# aggregate per run\n",
    "agg = d.groupby(group_id).apply(lambda g: pd.Series({\n",
    "    \"start\": g[\"start\"].iloc[0],\n",
    "    \"end\": g[\"end\"].iloc[-1],\n",
    "    # spoken time only (sum of segment durations; gaps excluded)\n",
    "    \"talk_duration\": float(g[\"duration\"].sum()),\n",
    "    # optional: wall-clock span (includes gaps) if you want it\n",
    "    \"span_duration\": float(g[\"end\"].iloc[-1] - g[\"start\"].iloc[0]),\n",
    "    \"who\": g[\"who\"].iloc[0],\n",
    "    # keep the first cluster label in the run (you can join all if you want)\n",
    "    \"cluster_first\": g[\"cluster\"].iloc[0],\n",
    "    # confidence summaries\n",
    "    \"cosine_sim_mean\": float(g[\"cosine_sim\"].mean()),\n",
    "    \"cosine_sim_min\": float(g[\"cosine_sim\"].min()),\n",
    "    \"segments_merged\": int(len(g)),\n",
    "})).reset_index(drop=True)\n",
    "\n",
    "# add gaps info\n",
    "gaps_info = d.groupby(group_id).apply(run_gaps).reset_index(drop=True)\n",
    "df_compact = pd.concat([agg, gaps_info], axis=1)\n",
    "\n",
    "# nice ordering\n",
    "df_compact = df_compact[\n",
    "    [\"start\",\"end\",\"talk_duration\",\"span_duration\",\n",
    "     \"pauses\",\"gap_total\",\"segments_merged\",\n",
    "     \"who\",\"cluster_first\",\"cosine_sim_mean\",\"cosine_sim_min\"]\n",
    "].sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "display(df_named.head(10))    # original detailed segments (unchanged)\n",
    "display(df_compact)  # new compacted view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "509dc0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>talk_duration</th>\n",
       "      <th>span_duration</th>\n",
       "      <th>pauses</th>\n",
       "      <th>gap_total</th>\n",
       "      <th>segments_merged</th>\n",
       "      <th>cosine_sim_mean</th>\n",
       "      <th>cosine_sim_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>71.111905</td>\n",
       "      <td>77.438571</td>\n",
       "      <td>5.559048</td>\n",
       "      <td>6.326667</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>0.767619</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>0.748243</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>50.219499</td>\n",
       "      <td>51.827323</td>\n",
       "      <td>5.736545</td>\n",
       "      <td>6.755280</td>\n",
       "      <td>2.357359</td>\n",
       "      <td>1.084440</td>\n",
       "      <td>2.357359</td>\n",
       "      <td>0.045349</td>\n",
       "      <td>0.047413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>3.780000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.560000</td>\n",
       "      <td>29.090000</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.120000</td>\n",
       "      <td>68.240000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>3.770000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754333</td>\n",
       "      <td>0.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>116.540000</td>\n",
       "      <td>117.890000</td>\n",
       "      <td>6.780000</td>\n",
       "      <td>7.880000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.781667</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156.710000</td>\n",
       "      <td>168.160000</td>\n",
       "      <td>20.640000</td>\n",
       "      <td>24.360000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.821500</td>\n",
       "      <td>0.799000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            start         end  talk_duration  span_duration     pauses  \\\n",
       "count   21.000000   21.000000      21.000000      21.000000  21.000000   \n",
       "mean    71.111905   77.438571       5.559048       6.326667   1.571429   \n",
       "std     50.219499   51.827323       5.736545       6.755280   2.357359   \n",
       "min      0.010000    3.780000       1.120000       1.120000   0.000000   \n",
       "25%     27.560000   29.090000       1.610000       1.610000   0.000000   \n",
       "50%     67.120000   68.240000       3.170000       3.770000   0.000000   \n",
       "75%    116.540000  117.890000       6.780000       7.880000   3.000000   \n",
       "max    156.710000  168.160000      20.640000      24.360000   8.000000   \n",
       "\n",
       "       gap_total  segments_merged  cosine_sim_mean  cosine_sim_min  \n",
       "count  21.000000        21.000000        21.000000       21.000000  \n",
       "mean    0.767619         2.571429         0.748243        0.722000  \n",
       "std     1.084440         2.357359         0.045349        0.047413  \n",
       "min     0.000000         1.000000         0.662000        0.651000  \n",
       "25%     0.000000         1.000000         0.722000        0.677000  \n",
       "50%     0.000000         1.000000         0.754333        0.742000  \n",
       "75%     1.100000         4.000000         0.781667        0.763000  \n",
       "max     3.720000         9.000000         0.821500        0.799000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compact.loc[~(df_compact['who'] == 'Unknown')].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b368d397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>talk_duration</th>\n",
       "      <th>span_duration</th>\n",
       "      <th>pauses</th>\n",
       "      <th>gap_total</th>\n",
       "      <th>segments_merged</th>\n",
       "      <th>cosine_sim_mean</th>\n",
       "      <th>cosine_sim_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>76.213333</td>\n",
       "      <td>77.643333</td>\n",
       "      <td>1.26000</td>\n",
       "      <td>1.430000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.536467</td>\n",
       "      <td>0.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>49.896556</td>\n",
       "      <td>50.504558</td>\n",
       "      <td>0.66674</td>\n",
       "      <td>1.120886</td>\n",
       "      <td>0.798809</td>\n",
       "      <td>0.505272</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>0.133737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.910000</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>0.44000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.445000</td>\n",
       "      <td>32.465000</td>\n",
       "      <td>0.77000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479750</td>\n",
       "      <td>0.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>65.900000</td>\n",
       "      <td>66.660000</td>\n",
       "      <td>1.12000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>116.475000</td>\n",
       "      <td>117.620000</td>\n",
       "      <td>1.48500</td>\n",
       "      <td>1.485000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>168.630000</td>\n",
       "      <td>173.520000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.890000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.890000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            start         end  talk_duration  span_duration     pauses  \\\n",
       "count   15.000000   15.000000       15.00000      15.000000  15.000000   \n",
       "mean    76.213333   77.643333        1.26000       1.430000   0.266667   \n",
       "std     49.896556   50.504558        0.66674       1.120886   0.798809   \n",
       "min     12.910000   14.690000        0.44000       0.440000   0.000000   \n",
       "25%     31.445000   32.465000        0.77000       0.770000   0.000000   \n",
       "50%     65.900000   66.660000        1.12000       1.120000   0.000000   \n",
       "75%    116.475000  117.620000        1.48500       1.485000   0.000000   \n",
       "max    168.630000  173.520000        3.00000       4.890000   3.000000   \n",
       "\n",
       "       gap_total  segments_merged  cosine_sim_mean  cosine_sim_min  \n",
       "count  15.000000        15.000000        15.000000       15.000000  \n",
       "mean    0.170000         1.333333         0.536467        0.504000  \n",
       "std     0.505272         0.816497         0.085185        0.133737  \n",
       "min     0.000000         1.000000         0.353000        0.217000  \n",
       "25%     0.000000         1.000000         0.479750        0.417500  \n",
       "50%     0.000000         1.000000         0.546000        0.546000  \n",
       "75%     0.000000         1.000000         0.612000        0.612000  \n",
       "max     1.890000         4.000000         0.637000        0.637000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compact.loc[df_compact['who'] == 'Unknown'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01f1a276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   start            36 non-null     float64\n",
      " 1   end              36 non-null     float64\n",
      " 2   talk_duration    36 non-null     float64\n",
      " 3   span_duration    36 non-null     float64\n",
      " 4   pauses           36 non-null     float64\n",
      " 5   gap_total        36 non-null     float64\n",
      " 6   segments_merged  36 non-null     int64  \n",
      " 7   who              36 non-null     object \n",
      " 8   cluster_first    36 non-null     object \n",
      " 9   cosine_sim_mean  36 non-null     float64\n",
      " 10  cosine_sim_min   36 non-null     float64\n",
      "dtypes: float64(8), int64(1), object(2)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_compact.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e87a2",
   "metadata": {},
   "source": [
    "### AUDIT: See how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32f8c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From raw segments:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who</th>\n",
       "      <th>seconds</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004-Claire-CS-LEAD--Hope</td>\n",
       "      <td>38.66</td>\n",
       "      <td>28.501917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005-Paul-FLEET--Mark</td>\n",
       "      <td>29.84</td>\n",
       "      <td>21.999410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>24.98</td>\n",
       "      <td>18.416396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>23.26</td>\n",
       "      <td>17.148334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>18.90</td>\n",
       "      <td>13.933943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         who  seconds    percent\n",
       "0   004-Claire-CS-LEAD--Hope    38.66  28.501917\n",
       "1       005-Paul-FLEET--Mark    29.84  21.999410\n",
       "2     002-Sam-COACH--Cassidy    24.98  18.416396\n",
       "3  003-Charlie-OWNER--Archer    23.26  17.148334\n",
       "4                    Unknown    18.90  13.933943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From compacted runs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who</th>\n",
       "      <th>seconds</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004-Claire-CS-LEAD--Hope</td>\n",
       "      <td>38.66</td>\n",
       "      <td>28.501917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005-Paul-FLEET--Mark</td>\n",
       "      <td>29.84</td>\n",
       "      <td>21.999410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002-Sam-COACH--Cassidy</td>\n",
       "      <td>24.98</td>\n",
       "      <td>18.416396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003-Charlie-OWNER--Archer</td>\n",
       "      <td>23.26</td>\n",
       "      <td>17.148334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>18.90</td>\n",
       "      <td>13.933943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         who  seconds    percent\n",
       "0   004-Claire-CS-LEAD--Hope    38.66  28.501917\n",
       "1       005-Paul-FLEET--Mark    29.84  21.999410\n",
       "2     002-Sam-COACH--Cassidy    24.98  18.416396\n",
       "3  003-Charlie-OWNER--Archer    23.26  17.148334\n",
       "4                    Unknown    18.90  13.933943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def summarize_speakers(df, time_col=\"duration\", label_col=\"who\"):\n",
    "    \"\"\"\n",
    "    Summarize how much each unique speaker talked.\n",
    "    df: DataFrame with a column for time and a column for speaker labels.\n",
    "    time_col: which col to sum (use 'duration' from df_named, or 'talk_duration' from df_compact)\n",
    "    label_col: which col holds speaker names (default 'who')\n",
    "    \"\"\"\n",
    "    total = df[time_col].sum()\n",
    "    summary = (\n",
    "        df.groupby(label_col)[time_col]\n",
    "          .sum()\n",
    "          .reset_index()\n",
    "          .rename(columns={time_col: \"seconds\"})\n",
    "    )\n",
    "    summary[\"percent\"] = 100 * summary[\"seconds\"] / total\n",
    "    summary = summary.sort_values(\"seconds\", ascending=False).reset_index(drop=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "summary_named   = summarize_speakers(df_named, time_col=\"duration\")\n",
    "summary_compact = summarize_speakers(df_compact, time_col=\"talk_duration\")\n",
    "\n",
    "print(\"From raw segments:\")\n",
    "display(summary_named)\n",
    "\n",
    "print(\"From compacted runs:\")\n",
    "display(summary_compact)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
